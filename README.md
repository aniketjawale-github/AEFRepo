ğŸš€ Azure EdgeFlow Data Engineering Project
ğŸ“Œ Overview
This project is a complete end-to-end Azure Data Engineering solution designed to simulate real-world enterprise data pipelines using modern Azure services. The project demonstrates data ingestion from external APIs, multi-layered storage using Medallion Architecture (Bronze â†’ Silver â†’ Gold), transformation via Azure Databricks, and reporting integration with Power BI.

The project is based on practical, real-time scenarios asked in interviews and implements automation, dynamic pipelines, and parameterization using low/no-code Azure tools.

ğŸ§° Tools & Technologies
Azure Data Factory (ADF) â€“ ETL orchestration & dynamic pipeline creation

Azure Data Lake Storage Gen2 â€“ Raw/Silver/Gold zone architecture

Azure Databricks â€“ Data transformation using PySpark

Azure Synapse Analytics â€“ Data warehousing and SQL analytics

Power BI â€“ Dashboarding and reporting

GitHub â€“ Source data access via public API

JSON â€“ Parameter config file for dynamic ADF pipelines

ğŸ—ï¸ Architecture
pgsql
Copy
Edit
                 +-------------------------+
                 |     GitHub API (CSV)    |
                 +-----------+-------------+
                             |
                             â–¼
                    [Azure Data Factory]
                    (Dynamic Copy Activity)
                             |
           +----------------+------------------+
           |                                   |
           â–¼                                   â–¼
     Bronze Zone (Raw) â†’ ADF â†’ Silver Zone (Cleaned) â†’ Databricks â†’ Gold Zone (Modeled)
                                                    |
                                                    â–¼
                                           Azure Synapse Analytics
                                                    |
                                                    â–¼
                                                Power BI

âš™ï¸ Features
âœ… Ingest data from GitHub REST API using ADF (via HTTP linked service)

âœ… Store raw data in Bronze layer using ADLS Gen2

âœ… Use Dynamic Pipelines in ADF with parameterized JSON input (for multiple files)

âœ… Transform data in Databricks (PySpark) and write to Silver layer

âœ… Load transformed data into Azure Synapse Analytics (Gold layer)

âœ… Connect Power BI to Synapse for real-time reporting

âœ… Use of Medallion Architecture (Bronze â†’ Silver â†’ Gold)

âœ… Followed real-world DevOps-style project structure

ğŸš€ Setup Instructions
1. Prerequisites
Azure account (Free Tier works)
GitHub account
VS Code (recommended)
Basic knowledge of Azure Portal, Python, and SQL

2. Create Resources
Create:

Azure Resource Group

Azure Data Lake Gen2 with Hierarchical Namespace

Azure Data Factory

Azure Databricks workspace

Azure Synapse workspace

Power BI Desktop (optional)

3. Pipeline Steps
Phase 1: Ingest Data via ADF (Static & Dynamic Pipelines)
Use Copy Activity to pull data from GitHub API to Bronze container

Configure Linked Services for HTTP & ADLS

Set up parameterized JSON and use ForEach Activity for dynamic ingestion

Phase 2: Transform with Databricks
Read Bronze data into Spark DataFrames

Apply transformation logic

Write output to Silver Zone in ADLS

Phase 3: Model in Synapse
Create Fact and Dimension tables in Synapse

Load Silver data into Synapse tables

Phase 4: Visualize with Power BI
Connect Power BI to Synapse SQL endpoint

Build interactive dashboards

